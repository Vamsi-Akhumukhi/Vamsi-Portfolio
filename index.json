[{"categories":["LeetCode"],"content":"Description Given a string s containing only lowercase letters, continuously remove adjacent characters that are the same and return the result. ","date":"2021-02-28","objectID":"/leetcode/1047.-remove-all-adjacent-duplicates-in-string/:1:0","tags":["Python","Facebook"],"title":"1047. Remove All Adjacent Duplicates In String","uri":"/leetcode/1047.-remove-all-adjacent-duplicates-in-string/"},{"categories":["LeetCode"],"content":"Test cases Example 1 Input: s = \"abbcccccaa\" Output: \"ca\" Example 2 Input: s = \"abccba\" Output: \"\" Example 3 Input: s = \"mbccbefddfe\" Output: \"m\" ","date":"2021-02-28","objectID":"/leetcode/1047.-remove-all-adjacent-duplicates-in-string/:2:0","tags":["Python","Facebook"],"title":"1047. Remove All Adjacent Duplicates In String","uri":"/leetcode/1047.-remove-all-adjacent-duplicates-in-string/"},{"categories":["LeetCode"],"content":"Solution We start with a result stack, go through all the characters in the string S one by one. If the current character and the previous character in the result are the same. We pop them as they are adjacent duplicate pair. If the next character is different, append it to the end of the result ","date":"2021-02-28","objectID":"/leetcode/1047.-remove-all-adjacent-duplicates-in-string/:3:0","tags":["Python","Facebook"],"title":"1047. Remove All Adjacent Duplicates In String","uri":"/leetcode/1047.-remove-all-adjacent-duplicates-in-string/"},{"categories":["LeetCode"],"content":"Code class Solution: def removeDuplicates(self, S: str) -\u003e str: res = [] for char in S: if res and res[-1] == char: res.pop() else: res.append(char) return \"\".join(res) Time Complexity = O(N) Space Complexity = O(N) ","date":"2021-02-28","objectID":"/leetcode/1047.-remove-all-adjacent-duplicates-in-string/:4:0","tags":["Python","Facebook"],"title":"1047. Remove All Adjacent Duplicates In String","uri":"/leetcode/1047.-remove-all-adjacent-duplicates-in-string/"},{"categories":["LeetCode"],"content":" Given two strings s and t, which represents a sequence of keystrokes, where # denotes a backspace, return whether or not the sequences produce the same result. Example 1 Input: s = \"ABC#\", t = \"CD##AB\" Output: true Example 2 Input: s = \"como#pur#ter\", t = \"computer\" Output: true Example 3 Input: \"cof#dim#ng\", t = \"coding\" Output: false class Solution: def backspaceCompare(self, S: str, T: str) -\u003e bool: def build(S): res = [] for char in S: if char != '#': res.append(char) # pop only if the result is not empty elif res: res.pop() return ''.join(res) #if we dont join the result will be in a list. return build(S) == build(T) Time Complexity = O(m+n), where m and n represents the length of the strings S and T respectively Space Complexity = O(1) ","date":"2021-02-28","objectID":"/leetcode/844.-backspace-string-compare/:0:0","tags":["Python"],"title":"844. Backspace String Compare","uri":"/leetcode/844.-backspace-string-compare/"},{"categories":["LeetCode"],"content":" Given the head of a linked list, remove the nth node from the end of the list and return its head. Follow up: Could you do this in one pass? Example 1 Input: head = [1,2,3,4,5,6], n = 2 Output: [1,2,3,4,6] Example 2 Input: head = [2], n = 1 Output: [] Example 3 Input: head = [1,2,3], n = 1 Output: [1,2] # Definition for singly-linked list. # class ListNode: # def __init__(self, val=0, next=None): # self.val = val # self.next = next class Solution: def removeNthFromEnd(self, head: ListNode, n: int) -\u003e ListNode: fast = slow = head for i in range(n): fast = fast.next if not fast: return head.next while fast.next: fast = fast.next slow = slow.next slow.next = slow.next.next return head Time Complexity = O(n) Space Complexity = O(1) ","date":"2021-02-20","objectID":"/leetcode/remove-nth-last-node-from-linked-list/remove-nth-last-node-from-linked-list/:0:0","tags":["Python"],"title":"19. Remove Nth Node From End of List","uri":"/leetcode/remove-nth-last-node-from-linked-list/remove-nth-last-node-from-linked-list/"},{"categories":["LeetCode"],"content":" Merge two sorted linked lists and return it as a sorted list. The list should be made by splicing together the nodes of the first two lists. Example 1 Input: l1 = [-1,2,4], l2 = [1,3,4] Output: [-1,1,2,3,4,4] Example 2 Input: l1 = [-1,4,5,7,8,9,10], l2 = [1,3,4] Output: [-1,1,3,4,4,5,7,8,9,10] Example 3 Input: l1 = [], l2 = [1,3,4] Output: [1,3,4] class Solution: def mergeTwoLists(self, l1: ListNode, l2: ListNode) -\u003e ListNode: dummy = curr = ListNode(0) #current always points to the tail of the list if l1: curr.next = l1 elif l2: curr.next = l2 # 2 edge cases where any of them could be null while l1 and l2: if l1.val \u003c l2.val: # i need my curr to point to l1 curr.next = l1 #now that my curr is at l1, i will move l1 forward l1 = l1.next else: curr.next = l2 l2 = l2.next curr = curr.next #when both are equal curr.next = l1 or l2 return dummy.next Time Complexity = O(m+n), where m and n represents the length of the list L1 and L2 respectively Space Complexity = O(1) ","date":"2021-02-20","objectID":"/leetcode/merge-two-sorted-linked-lists/:0:0","tags":["Python"],"title":"21.  Merge Two Sorted Lists","uri":"/leetcode/merge-two-sorted-linked-lists/"},{"categories":["LeetCode"],"content":"Nothing is more important than the World Cup in Football(Soccer). I made this viz to show Germany's dominance in Football.","date":"2021-02-19","objectID":"/leetcode/detect-capital/","tags":["Python"],"title":"520. Detect Capital","uri":"/leetcode/detect-capital/"},{"categories":["LeetCode"],"content":" Given a word, you need to judge whether the usage of capitals in it is right or not. We define the usage of capitals in a word to be right when one of the following cases holds: All letters in this word are capitals, like “AMERICA”. All letters in this word are not capitals, like “vamsi”. Only the first letter in this word is capital, like “Predator”. class Solution: def detectCapitalUse(self, word: str) -\u003e bool: return (word[0] == word[0].upper() and (word[1:] == word[1:].lower() or word[1:] == word[1:].upper())) or (word == word.lower()) ","date":"2021-02-19","objectID":"/leetcode/detect-capital/:0:0","tags":["Python"],"title":"520. Detect Capital","uri":"/leetcode/detect-capital/"},{"categories":["Projects"],"content":"Nothing is more important than the World Cup in Football(Soccer). I made this viz to show Germany's dominance in Football.","date":"2021-01-11","objectID":"/posts/germany-world-cup/","tags":["Tableau"],"title":"Germany's Dominance in FIFA World Cup using Tableau","uri":"/posts/germany-world-cup/"},{"categories":["Projects"],"content":"Nothing is more important than the World Cup in Football(Soccer). I made this viz to show Germany’s dominance in Football. ","date":"2021-01-11","objectID":"/posts/germany-world-cup/:0:0","tags":["Tableau"],"title":"Germany's Dominance in FIFA World Cup using Tableau","uri":"/posts/germany-world-cup/"},{"categories":["Projects"],"content":"Analysis Here are my analysis: 16 times finished as top 3 out of 19 tournaments played. 4 times Champions, 4 times Runners-up and 4 times finished Third. Consistent in the last 80 years. Impressive record of 59 Wins, 13 Losses and 6 Draws i.e. 75.64% winning ratio. After each successful tournament, Germany had their worst performance in the following tournament, but with time they quickly bounced back to being champions again. Germany Finished as 22nd in 2018. By my analysis I can say that Germany will be Champions soon. Data is an art. Had a great time creating this art. The power of Tableau Software is impressive and mind blowing. . ","date":"2021-01-11","objectID":"/posts/germany-world-cup/:1:0","tags":["Tableau"],"title":"Germany's Dominance in FIFA World Cup using Tableau","uri":"/posts/germany-world-cup/"},{"categories":["Projects"],"content":"Viz \rEmbedded Analytics with Tableau Online\r\r\r\r\r\r\r\rvar divElement = document.getElementById('viz1612040935565');\rvar vizElement = divElement.getElementsByTagName('object')[0];\rif ( divElement.offsetWidth 800 ) { vizElement.style.width='1366px';vizElement.style.height='1527px';}\relse if ( divElement.offsetWidth 500 ) { vizElement.style.width='1366px';vizElement.style.height='1527px';} else { vizElement.style.width='100%';vizElement.style.height='5277px';}\rvar scriptElement = document.createElement('script');\rscriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';\rvizElement.parentNode.insertBefore(scriptElement, vizElement);\r\r\r","date":"2021-01-11","objectID":"/posts/germany-world-cup/:2:0","tags":["Tableau"],"title":"Germany's Dominance in FIFA World Cup using Tableau","uri":"/posts/germany-world-cup/"},{"categories":null,"content":"Lets see what happens now ","date":"2020-09-05","objectID":"/about/","tags":null,"title":"About Me","uri":"/about/"},{"categories":null,"content":"Okay it works Gone camping!  Be back soon. That is so funny! ","date":"2020-09-05","objectID":"/about/:1:0","tags":null,"title":"About Me","uri":"/about/"},{"categories":["Projects"],"content":"The aim of the following Project is to predict whether income of a person exceeds $50K/year based on Adult dataset . ","date":"2020-09-05","objectID":"/posts/machine-learning-final-project/","tags":["Linear Regression","SVM","SGD"],"title":"Machine Learning Algorithms (SVM, Log Regression and SGD) on Adult Dataset","uri":"/posts/machine-learning-final-project/"},{"categories":["Projects"],"content":"The aim of the following Project is to predict whether income of a person exceeds $50K/year based on Adult dataset. from google.colab import drive drive.mount('/content/drive') Mounted at /content/drive ","date":"2020-09-05","objectID":"/posts/machine-learning-final-project/:0:0","tags":["Linear Regression","SVM","SGD"],"title":"Machine Learning Algorithms (SVM, Log Regression and SGD) on Adult Dataset","uri":"/posts/machine-learning-final-project/"},{"categories":["Projects"],"content":"Importing the Libraries import numpy as np import pandas as pd import statsmodels.api as sm from math import exp import scipy.optimize as opt import matplotlib.animation as animation import math from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score, recall_score, precision_score from sklearn.utils import shuffle import matplotlib.pyplot as plt /usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead. import pandas.util.testing as tm fit = pd.read_csv('/content/drive/My Drive/Machine Learning/adult_data.csv',na_values='?') data=[fit] fit Age WorkClass Final_weight Education Education_num Martial_status Occupation Relationship Race Sex Capital_gain Capital_loss hours_per_week native_country Salary 0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States \u003c=50K 1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States \u003c=50K 2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States \u003c=50K 3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States \u003c=50K 4 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black Female 0 0 40 Cuba \u003c=50K … … … … … … … … … … … … … … … … 32556 27 Private 257302 Assoc-acdm 12 Married-civ-spouse Tech-support Wife White Female 0 0 38 United-States \u003c=50K 32557 40 Private 154374 HS-grad 9 Married-civ-spouse Machine-op-inspct Husband White Male 0 0 40 United-States \u003e50K 32558 58 Private 151910 HS-grad 9 Widowed Adm-clerical Unmarried White Female 0 0 40 United-States \u003c=50K 32559 22 Private 201490 HS-grad 9 Never-married Adm-clerical Own-child White Male 0 0 20 United-States \u003c=50K 32560 52 Self-emp-inc 287927 HS-grad 9 Married-civ-spouse Exec-managerial Wife White Female 15024 0 40 United-States \u003e50K fit['WorkClass'].value_counts() Private 22696 Self-emp-not-inc 2541 Local-gov 2093 ? 1836 State-gov 1298 Self-emp-inc 1116 Federal-gov 960 Without-pay 14 Never-worked 7 Name: WorkClass, dtype: int64 fit= fit.replace('[?]', np.nan, regex=True) print(fit[25:40]) Age WorkClass ... native_country Salary 25 56 Local-gov ... United-States \u003e50K 26 19 Private ... United-States \u003c=50K 27 54 NaN ... South \u003e50K 28 39 Private ... United-States \u003c=50K 29 49 Private ... United-States \u003c=50K 30 23 Local-gov ... United-States \u003c=50K 31 20 Private ... United-States \u003c=50K 32 45 Private ... United-States \u003c=50K 33 30 Federal-gov ... United-States \u003c=50K 34 22 State-gov ... United-States \u003c=50K 35 48 Private ... Puerto-Rico \u003c=50K 36 21 Private ... United-States \u003c=50K 37 19 Private ... United-States \u003c=50K 38 31 Private ... NaN \u003e50K 39 48 Self-emp-not-inc ... United-States \u003c=50K [15 rows x 15 columns] from collections import Counter # summarize the class distribution target = fit.values[:,-1] counter = Counter(target) for k,v in counter.items(): per = v / len(target) * 100 print('Class=%s, Count=%d, Percentage=%.3f%%' % (k, v, per)) Class= \u003c=50K, Count=24720, Percentage=75.919% Class= \u003e50K, Count=7841, Percentage=24.081% import seaborn as sns sns.heatmap(fit.isna(), cbar=True,) \u003cmatplotlib.axes._subplots.AxesSubplot at 0x7f455550f978\u003e fit = fit.dropna() fit Age WorkClass Final_weight Education Education_num Martial_status Occupation Relationship Race Sex Capital_gain Capital_loss hours_per_week native_country Salary 0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White Male 2174 0 40 United-States \u003c=50K 1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White Male 0 0 13 United-States \u003c=50K 2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White Male 0 0 40 United-States \u003c=50K 3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black Male 0 0 40 United-States \u003c=","date":"2020-09-05","objectID":"/posts/machine-learning-final-project/:1:0","tags":["Linear Regression","SVM","SGD"],"title":"Machine Learning Algorithms (SVM, Log Regression and SGD) on Adult Dataset","uri":"/posts/machine-learning-final-project/"},{"categories":["Projects"],"content":"Feature Engineering # Converting Salary to 0 and 1 salary_map={' \u003c=50K':1,' \u003e50K':0} fit['Salary']=fit['Salary'].map(salary_map).astype(int) fit['Salary'].head(10) 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 0 8 0 9 0 Name: Salary, dtype: int64 # Converting Sex to integer fit['Sex'] = fit['Sex'].map({' Male':1,' Female':0}).astype(int) print (fit.head(10)) print ((\"-\"*40)) print (fit.info()) Age WorkClass Final_weight ... hours_per_week native_country Salary 0 39 State-gov 77516 ... 40 United-States 1 1 50 Self-emp-not-inc 83311 ... 13 United-States 1 2 38 Private 215646 ... 40 United-States 1 3 53 Private 234721 ... 40 United-States 1 4 28 Private 338409 ... 40 Cuba 1 5 37 Private 284582 ... 40 United-States 1 6 49 Private 160187 ... 16 Jamaica 1 7 52 Self-emp-not-inc 209642 ... 45 United-States 0 8 31 Private 45781 ... 50 United-States 0 9 42 Private 159449 ... 40 United-States 0 [10 rows x 15 columns] ---------------------------------------- \u003cclass 'pandas.core.frame.DataFrame'\u003e Int64Index: 30162 entries, 0 to 32560 Data columns (total 15 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 Age 30162 non-null int64 1 WorkClass 30162 non-null object 2 Final_weight 30162 non-null int64 3 Education 30162 non-null object 4 Education_num 30162 non-null int64 5 Martial_status 30162 non-null object 6 Occupation 30162 non-null object 7 Relationship 30162 non-null object 8 Race 30162 non-null object 9 Sex 30162 non-null int64 10 Capital_gain 30162 non-null int64 11 Capital_loss 30162 non-null int64 12 hours_per_week 30162 non-null int64 13 native_country 30162 non-null object 14 Salary 30162 non-null int64 dtypes: int64(8), object(7) memory usage: 3.7+ MB None # Categorize between US and Non -Us fit['native_country'].unique() array([' United-States', ' Cuba', ' Jamaica', ' India', ' Mexico', ' Puerto-Rico', ' Honduras', ' England', ' Canada', ' Germany', ' Iran', ' Philippines', ' Poland', ' Columbia', ' Cambodia', ' Thailand', ' Ecuador', ' Laos', ' Taiwan', ' Haiti', ' Portugal', ' Dominican-Republic', ' El-Salvador', ' France', ' Guatemala', ' Italy', ' China', ' South', ' Japan', ' Yugoslavia', ' Peru', ' Outlying-US(Guam-USVI-etc)', ' Scotland', ' Trinadad\u0026Tobago', ' Greece', ' Nicaragua', ' Vietnam', ' Hong', ' Ireland', ' Hungary', ' Holand-Netherlands'], dtype=object) data=[fit] for dataset in data: dataset.loc[dataset['native_country'] != ' United-States', 'native_country'] = 'Non-US' dataset.loc[dataset['native_country'] == ' United-States', 'native_country'] = 'US' fit Age WorkClass Final_weight Education Education_num Martial_status Occupation Relationship Race Sex Capital_gain Capital_loss hours_per_week native_country Salary 0 39 State-gov 77516 Bachelors 13 Never-married Adm-clerical Not-in-family White 1 2174 0 40 US 1 1 50 Self-emp-not-inc 83311 Bachelors 13 Married-civ-spouse Exec-managerial Husband White 1 0 0 13 US 1 2 38 Private 215646 HS-grad 9 Divorced Handlers-cleaners Not-in-family White 1 0 0 40 US 1 3 53 Private 234721 11th 7 Married-civ-spouse Handlers-cleaners Husband Black 1 0 0 40 US 1 4 28 Private 338409 Bachelors 13 Married-civ-spouse Prof-specialty Wife Black 0 0 0 40 Non-US 1 … … … … … … … … … … … … … … … … 32556 27 Private 257302 Assoc-acdm 12 Married-civ-spouse Tech-support Wife White 0 0 0 38 US 1 32557 40 Private 154374 HS-grad 9 Married-civ-spouse Machine-op-inspct Husband White 1 0 0 40 US 0 32558 58 Private 151910 HS-grad 9 Widowed Adm-clerical Unmarried White 0 0 0 40 US 1 32559 22 Private 201490 HS-grad 9 Never-married Adm-clerical Own-child White 1 0 0 20 US 1 32560 52 Self-emp-inc 287927 HS-grad 9 Married-civ-spouse Exec-managerial Wife White 0 15024 0 40 US 0 30162 rows × 15 columns fit['native_country'] = fit['native_country'].map({'US':1,'Non-US':0}).astype(int) fit['native_country'].head(10) 0 1 1 1 2 1 3 1 4 0 5 1 6 0 7 1 8 1 9 1 Name: native_country, dtype: int64 fit['Martial_status'] = fit['Martial_status'].replace([' Divorced', ' Married-spouse-absent', ' Never-married', '","date":"2020-09-05","objectID":"/posts/machine-learning-final-project/:2:0","tags":["Linear Regression","SVM","SGD"],"title":"Machine Learning Algorithms (SVM, Log Regression and SGD) on Adult Dataset","uri":"/posts/machine-learning-final-project/"},{"categories":["Projects"],"content":"Normalization from sklearn import preprocessing # Data Normalization normalized_X = preprocessing.scale(x) X= normalized_X # Normalised Data X= np.asarray(X) X array([[ 0.04279571, 1.12891838, 0.93606249, ..., -0.22284679, -0.07773411, 0.31087053], [ 0.88028814, 1.12891838, -1.06830474, ..., -0.22284679, -2.3315307 , 0.31087053], [-0.03333996, -0.4397382 , 0.93606249, ..., -0.22284679, -0.07773411, 0.31087053], ..., [ 1.48937355, -0.4397382 , 0.93606249, ..., -0.22284679, -0.07773411, 0.31087053], [-1.25151078, -0.4397382 , 0.93606249, ..., -0.22284679, -1.74721307, 0.31087053], [ 1.0325595 , -0.4397382 , -1.06830474, ..., -0.22284679, -0.07773411, 0.31087053]]) y=np.asarray(y) y array([[1], [1], [1], ..., [1], [1], [0]]) X=np.hstack(((np.ones(len(X))).reshape(-1,1),X)) X array([[ 1. , 0.04279571, 1.12891838, ..., -0.22284679, -0.07773411, 0.31087053], [ 1. , 0.88028814, 1.12891838, ..., -0.22284679, -2.3315307 , 0.31087053], [ 1. , -0.03333996, -0.4397382 , ..., -0.22284679, -0.07773411, 0.31087053], ..., [ 1. , 1.48937355, -0.4397382 , ..., -0.22284679, -0.07773411, 0.31087053], [ 1. , -1.25151078, -0.4397382 , ..., -0.22284679, -1.74721307, 0.31087053], [ 1. , 1.0325595 , -0.4397382 , ..., -0.22284679, -0.07773411, 0.31087053]]) X_train,X_test,y_train,y_test = train_test_split(X,y, test_size=0.25, random_state=42) len(X_train), len(X_test) (22621, 7541) def compute_cost(W, X, Y): # calculate hinge loss N = X.shape[0] distances = 1 - Y * (np.dot(X, W)) distances[distances \u003c 0] = 0 # equivalent to max(0, distance) hinge_loss = regularization_strength * (np.sum(distances) / N) # calculate cost cost = 1 / 2 * np.dot(W, W) + hinge_loss return cost def calculate_cost_gradient(W, X_batch, Y_batch): # if only one example is passed (eg. in case of SGD) if type(Y_batch) == np.float64: Y_batch = np.array([Y_batch]) X_batch = np.array([X_batch]) # gives multidimensional array distance = 1 - (Y_batch * np.dot(X_batch, W)) dw = np.zeros(len(W)) for ind, d in enumerate(distance): if max(0, d) == 0: di = W else: di = W - (regularization_strength * Y_batch[ind] * X_batch[ind]) dw += di dw = dw/len(Y_batch) # average return dw def sgd(features, outputs): max_epochs = 100 weights = np.array([0,0,0,0,0,0,0,0,0,0,0]) nth = 0 prev_cost = float(\"inf\") cost_threshold = 0.01 # in percent J_history=[] # stochastic gradient descent for epoch in range(1, max_epochs): # shuffle to prevent repeating update cycles X, Y = shuffle(features, outputs) for ind, x in enumerate(X): ascent = calculate_cost_gradient(weights, x, Y[ind]) weights = weights - (learning_rate * ascent) # convergence check on 2^nth epoch if epoch == 2 ** nth or epoch == max_epochs - 1: cost = compute_cost(weights, features, outputs) print(\"Epoch is: {} and Cost is: {}\".format(epoch, cost)) # stoppage criterion if abs(prev_cost - cost) \u003c cost_threshold * prev_cost: return weights prev_cost = cost nth += 1 J_history.append(cost) return weights,J_history regularization_strength = 100 learning_rate = 0.5 # Training the model W,J_history=sgd(X_train,y_train) Epoch is: 1 and Cost is: 64950616.66954408 Epoch is: 2 and Cost is: 85814516.7868933 Epoch is: 4 and Cost is: 2244751.1160014216 Epoch is: 8 and Cost is: 45447118.64829404 Epoch is: 16 and Cost is: 14714540.629473215 Epoch is: 32 and Cost is: 4344601.036239334 Epoch is: 64 and Cost is: 37090385.394482166 Epoch is: 99 and Cost is: 46900813.34246465 W array([7.06868769, 7.06868769, 7.06868769]) print(\"training the model...\") y_train_predicted = np.array([]) for i in range(X_train.shape[0]): yp = np.sign(np.dot(X_train[i], W)) y_train_predicted = np.append(y_train_predicted, yp) training the model... y_train_predicted array([ 1., 1., -1., ..., 1., 1., 1.]) y_test_predicted = np.array([]) for i in range(X_test.shape[0]): yp = np.sign(np.dot(X_test[i], W)) y_test_predicted = np.append(y_test_predicted, yp) print(\"accuracy on test dataset: {}\".format(accuracy_score(y_test, y_test_predicted))) accuracy on test dataset: 0.4894576316138443 from s","date":"2020-09-05","objectID":"/posts/machine-learning-final-project/:3:0","tags":["Linear Regression","SVM","SGD"],"title":"Machine Learning Algorithms (SVM, Log Regression and SGD) on Adult Dataset","uri":"/posts/machine-learning-final-project/"},{"categories":["Projects"],"content":"PCA from sklearn.decomposition import PCA pca = PCA(n_components=2) principalComponents = pca.fit_transform(X) principalDf = pd.DataFrame(data = principalComponents , columns = ['principal component 1', 'principal component 2']) principalDf array([[ 0.3611508 , -1.06582651], [ 0.48002773, 0.03450933], [-0.41751607, -0.62298376], ..., [ 0.09665751, 1.5659688 ], [-1.40701567, -1.13857703], [ 1.0930337 , 1.08675433]]) principalDf = np.asarray(principalDf) from sklearn.svm import SVC model = SVC(kernel='linear', C=100,coef0=13.19098827) model.fit(principalDf[0:22621], y[0:22621]) /usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=13.19098827, decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) ","date":"2020-09-05","objectID":"/posts/machine-learning-final-project/:4:0","tags":["Linear Regression","SVM","SGD"],"title":"Machine Learning Algorithms (SVM, Log Regression and SGD) on Adult Dataset","uri":"/posts/machine-learning-final-project/"},{"categories":["Projects"],"content":"SVM on Training Dataset ax = plt.gca() color = ['green' if c == 0 else 'red' for c in y[0:22621]] plt.scatter(principalDf[0:22621, 0], principalDf[0:22621, 1], c=color) xlim = ax.get_xlim() ylim = ax.get_ylim() xx = np.linspace(xlim[0], xlim[1], 30) yy = np.linspace(ylim[0], ylim[1], 30) YY, XX = np.meshgrid(yy, xx) xy = np.vstack([XX.ravel(), YY.ravel()]).T Z = model.decision_function(xy).reshape(XX.shape) ax.contour(XX, YY, Z, colors=['green','blue','red'], levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--']) ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k') ax.set_ylabel('Estimated Salary') ax.set_xlabel('X (Predictor)') ax.set_title('SVM Classification on training dataset') plt.show() model = SVC(kernel='linear', C=100,coef0=13.19098827) model.fit(principalDf[22621:], y[22621:]) /usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel(). y = column_or_1d(y, warn=True) SVC(C=100, break_ties=False, cache_size=200, class_weight=None, coef0=13.19098827, decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear', max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) ","date":"2020-09-05","objectID":"/posts/machine-learning-final-project/:5:0","tags":["Linear Regression","SVM","SGD"],"title":"Machine Learning Algorithms (SVM, Log Regression and SGD) on Adult Dataset","uri":"/posts/machine-learning-final-project/"},{"categories":["Projects"],"content":"SVM on Testing Dataset ax = plt.gca() color = ['green' if c == 0 else 'red' for c in y[22621:]] plt.scatter(principalDf[22621:, 0], principalDf[22621:, 1], c=color) xlim = ax.get_xlim() ylim = ax.get_ylim() xx = np.linspace(xlim[0], xlim[1], 30) yy = np.linspace(ylim[0], ylim[1], 30) YY, XX = np.meshgrid(yy, xx) xy = np.vstack([XX.ravel(), YY.ravel()]).T Z = model.decision_function(xy).reshape(XX.shape) ax.contour(XX, YY, Z, colors=['green','blue','red'], levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--']) ax.scatter(model.support_vectors_[:, 0], model.support_vectors_[:, 1], s=100, linewidth=1, facecolors='none', edgecolors='k') ax.set_ylabel('Estimated Salary') ax.set_xlabel('X (Predictor)') ax.set_title('SVM Classification on testing dataset') plt.show() ","date":"2020-09-05","objectID":"/posts/machine-learning-final-project/:6:0","tags":["Linear Regression","SVM","SGD"],"title":"Machine Learning Algorithms (SVM, Log Regression and SGD) on Adult Dataset","uri":"/posts/machine-learning-final-project/"},{"categories":["Projects"],"content":"Logistic regression X array([[ 1. , 0.04279571, 1.12891838, ..., -0.22284679, -0.07773411, 0.31087053], [ 1. , 0.88028814, 1.12891838, ..., -0.22284679, -2.3315307 , 0.31087053], [ 1. , -0.03333996, -0.4397382 , ..., -0.22284679, -0.07773411, 0.31087053], ..., [ 1. , 1.48937355, -0.4397382 , ..., -0.22284679, -0.07773411, 0.31087053], [ 1. , -1.25151078, -0.4397382 , ..., -0.22284679, -1.74721307, 0.31087053], [ 1. , 1.0325595 , -0.4397382 , ..., -0.22284679, -0.07773411, 0.31087053]]) m , n = X.shape[0], X.shape[1] X= np.append(np.ones((m,1)),X,axis=1) X array([[ 1. , 0.04279571, 1.12891838, ..., -0.22284679, -0.07773411, 0.31087053], [ 1. , 0.88028814, 1.12891838, ..., -0.22284679, -2.3315307 , 0.31087053], [ 1. , -0.03333996, -0.4397382 , ..., -0.22284679, -0.07773411, 0.31087053], ..., [ 1. , 1.48937355, -0.4397382 , ..., -0.22284679, -0.07773411, 0.31087053], [ 1. , -1.25151078, -0.4397382 , ..., -0.22284679, -1.74721307, 0.31087053], [ 1. , 1.0325595 , -0.4397382 , ..., -0.22284679, -0.07773411, 0.31087053]]) from sklearn.linear_model import LogisticRegression from sklearn import metrics X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42) logreg = LogisticRegression() logreg.fit(X_train, y_train) /usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py:760: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().y = column_or_1d(y, warn=True) LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True, intercept_scaling=1, l1_ratio=None, max_iter=100, multi_class='auto', n_jobs=None, penalty='l2', random_state=None, solver='lbfgs', tol=0.0001, verbose=0, warm_start=False) def sigmoid(z): return 1/ (1 + np.exp(-z)) def costFunction(theta, X, y): m=len(y) predictions = sigmoid(np.dot(X,theta)) error = (-y * np.log(predictions)) - ((1-y)*np.log(1-predictions)) cost = 1/m * sum(error) grad = 1/m * np.dot(X.transpose(),(predictions - y)) return cost[0] , grad def gradientDescent(X,y,theta,alpha,num_iters): m=len(y) J_history =[] for i in range(num_iters): cost, grad = costFunction(theta,X,y) theta = theta - (alpha * grad) J_history.append(cost) return theta , J_history theta_result,cost_history=gradientDescent(X_train,y_train,initial_theta,alpha=0.5,num_iters=1000) theta_result array([[ 1.86802546], [-0.38192732], [-0.93415412], [ 1.08991493], [ 0.19253997], [ 0.08182571], [-0.09390707], [-0.45759634], [-0.22849672], [-0.34343411], [-0.06609421]]) %matplotlib inline import matplotlib.pyplot as plt fig = plt.figure(figsize = (14,7)) ax = fig.add_subplot(1,1,1) ax.plot(cost_history,marker = '*') ax.set_ylabel('Cost History') ax.set_xlabel('Number of Iterations') ax.set_title('Cost History vs Number of Iterations') Text(0.5, 1.0, 'Cost History vs Number of Iterations') initial_theta= np.array([0, 0, 0,0,0,0,0,0,0,0,0]).reshape(-1,1) initial_theta array([[0], [0], [0], [0], [0], [0], [0], [0], [0], [0], [0]]) for i in range(len(X)): z = sigmoid(np.dot(X,theta_result)) initial_theta = np.zeros((n+1,1)) cost, grad= costFunction(initial_theta,X,y) print(\"Cost of initial theta is\",cost) print(\"Gradient at initial theta (zeros):\",grad) def classifierPredict(theta,X): predictions = X.dot(theta) return predictions\u003e0 p=classifierPredict(theta_result,X_test) print(\"Test Accuracy:\", sum(p==y_test)[0]) Test Accuracy: 6311 len(y_test) 7541 Question 4 newdata = fit.copy() abc = newdata[:10] abc.drop (['Salary'],axis=1,inplace=True) /usr/local/lib/python3.6/dist-packages/pandas/core/frame.py:3997: SettingWithCopyWarning: A value is trying to be set on a copy of a slice from a DataFrame See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy errors=errors, abc Age Education_num Martial_status Relationship Race Sex Capital_gain Capital_loss hours_per_week native_country 0 39 13 1 3 0 1 1 0 40 1 1 50 13 ","date":"2020-09-05","objectID":"/posts/machine-learning-final-project/:7:0","tags":["Linear Regression","SVM","SGD"],"title":"Machine Learning Algorithms (SVM, Log Regression and SGD) on Adult Dataset","uri":"/posts/machine-learning-final-project/"},{"categories":["Projects"],"content":" Cristiano Ronaldo's Journey to become one of the greatest footballer of all time ","date":"2020-06-11","objectID":"/posts/ronaldo/","tags":["Tableau"],"title":"Cristiano Ronaldo's Success","uri":"/posts/ronaldo/"},{"categories":["Projects"],"content":"Introduction A boy from Maderia to Madrid. Why ? To become the face of football. Ronaldo played 9 seasons with Real Madrid, from 2009/10 to 2018/19. During his 9 years amazing career with Real Madrid. He broke many records. So, I listed some of them after performing the analysis. ","date":"2020-06-11","objectID":"/posts/ronaldo/:1:0","tags":["Tableau"],"title":"Cristiano Ronaldo's Success","uri":"/posts/ronaldo/"},{"categories":["Projects"],"content":"Analysis He scored 450 goals. First player in the UEFA Champions League to reach 100 club goals. Forget about players scoring 34 goals during their career, Ronaldo scored 34 career Hat-tricks with Real Madrid alone. In a season, some clubs fail to reach 50+ goal mark, whereas, Ronaldo scored 61 goals in a season (2014/15). My love for football and passion for analytics lead to this amazing work. Anyone can learn skills, but applying the knowledge and loving your work leads to success. . ","date":"2020-06-11","objectID":"/posts/ronaldo/:2:0","tags":["Tableau"],"title":"Cristiano Ronaldo's Success","uri":"/posts/ronaldo/"},{"categories":["Projects"],"content":"Viz \rEmbedded Analytics with Tableau\r\r\r\r\r\r\r\rvar divElement = document.getElementById('viz1611979640118');\rvar vizElement = divElement.getElementsByTagName('object')[0];\rif ( divElement.offsetWidth 800 )\r{ vizElement.style.width='1366px';vizElement.style.height='768px';}\relse if ( divElement.offsetWidth 500 )\r{ vizElement.style.width='1366px';vizElement.style.height='768px';}\relse { vizElement.style.width='100%';vizElement.style.height='1750px';}\rvar scriptElement = document.createElement('script');\rscriptElement.src = 'https://public.tableau.com/javascripts/api/viz_v1.js';\rvizElement.parentNode.insertBefore(scriptElement, vizElement);\r\r\r","date":"2020-06-11","objectID":"/posts/ronaldo/:3:0","tags":["Tableau"],"title":"Cristiano Ronaldo's Success","uri":"/posts/ronaldo/"}]